{
	"name": "CreateParquetTable",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4aacc607-65f4-4644-9287-c5bf212f559f"
			}
		},
		"metadata": {
			"saveOutput": false,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create an external Spark table based on existing parquet files on the data lake\r\n",
					"\r\n",
					"When executed, this notebook creates a shared metadata table in a Synapse lake database. This table is managed in Spark, but can also be queried using Synapse Serverless SQL. [Read more about shared metadata tables](https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table).\r\n",
					"\r\n",
					"The table created by this notebook is an external table, meaning that it is based on existing data on the data lake. Data is not copied or duplicated when creating the table. The notebook will create a database with the name of the data lake container, if it does not exist yet. If a table with the same name already exists, no new table will be created.\r\n",
					"\r\n",
					"This notebook is primarily meant to be run by the Consolidation_OneEntity pipeline. If you are running the notebook manually, make sure to uncomment and fill in the parameters in the next cell."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# These parameters are automatically provided by the calling pipeline. Uncomment and provide parameters if run manually!\r\n",
					"# container_name = ''\r\n",
					"# entity_name = ''"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Retrieve storage account name from Linked Service\r\n",
					"\r\n",
					"import re\r\n",
					"import json\r\n",
					"linked_service_name = 'AzureDataLakeStorage'\r\n",
					"linked_service_props = mssparkutils.credentials.getPropertiesAll(linked_service_name)\r\n",
					"ls_props_json = json.loads(linked_service_props)\r\n",
					"ls_endpoint = ls_props_json['Endpoint']\r\n",
					"print(f'Found endpoint {ls_endpoint}')\r\n",
					"re_match = re.search('https://(.*).dfs.core.windows.net', ls_endpoint)\r\n",
					"storage_account_name = re_match.group(1)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create database\r\n",
					"\r\n",
					"db_name = re.sub('[^a-zA-Z0-9_]', '_', container_name) # remove invalid characters and replace them with underscores\r\n",
					"query = f'CREATE DATABASE IF NOT EXISTS `{db_name}`'\r\n",
					"spark.sql(query)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create table\r\n",
					"\r\n",
					"table_name = re.sub('[^a-zA-Z0-9_]', '_', entity_name) # remove invalid characters and replace them with underscores\r\n",
					"location = f'abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/data/{entity_name}/'\r\n",
					"query = f'CREATE TABLE IF NOT EXISTS `{db_name}`.`{table_name}` USING Parquet LOCATION \"{location}\"'\r\n",
					"spark.sql(query)"
				],
				"execution_count": 2
			}
		]
	}
}